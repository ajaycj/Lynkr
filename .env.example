# Lynkr + Ollama Configuration Example
# Copy this file to .env and fill in your values

# ==============================================================================
# Model Provider Configuration
# ==============================================================================

# Primary model provider to use
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, ollama, llamacpp, lmstudio
# Default: databricks
# MODEL_PROVIDER=databricks

# ==============================================================================
# Ollama Configuration (Hybrid Routing)
# ==============================================================================

# Enable Ollama preference for simple requests
PREFER_OLLAMA=true

# Ollama model to use (must be compatible with tool calling)
# Options: qwen2.5-coder:latest, llama3.1, mistral-nemo, etc.
OLLAMA_MODEL=qwen2.5-coder:latest

# Fallback provider when primary provider fails or for complex requests
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai
# Note: Local providers (ollama, llamacpp, lmstudio) cannot be used as fallback
FALLBACK_PROVIDER=databricks

# Enable automatic fallback (true = transparent fallback, false = fail on provider error)
FALLBACK_ENABLED=true

# Max tools for routing to Ollama (requests with more tools go to cloud)
OLLAMA_MAX_TOOLS_FOR_ROUTING=3

# ==============================================================================
# Databricks Configuration (Required for Fallback)
# ==============================================================================

# DATABRICKS_API_BASE=https://your-workspace.cloud.databricks.com
# DATABRICKS_API_KEY=dapi1234567890abcdef

# ==============================================================================
# Azure Anthropic Configuration (Optional Alternative Fallback)
# ==============================================================================

# AZURE_ANTHROPIC_ENDPOINT=https://your-anthropic.openai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-azure-key

# ==============================================================================
# Azure OpenAI Configuration (Optional Alternative Fallback)
# ==============================================================================

# Azure OpenAI resource endpoint (from Azure Portal)
# Format: https://<your-resource-name>.openai.azure.com
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com

# Azure OpenAI API key (found in Azure Portal under Keys and Endpoint)
# AZURE_OPENAI_API_KEY=your-azure-openai-key

# Deployment name (the model deployment you created in Azure)
# This can point to any Azure OpenAI model: gpt-4o, gpt-5, gpt-5-codex, etc.
# Default: gpt-4o
# AZURE_OPENAI_DEPLOYMENT=gpt-4o

# API version (use latest stable version for best features)
# Default: 2024-08-01-preview
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

# ==============================================================================
# OpenAI Configuration (Direct OpenAI API)
# ==============================================================================

# OpenAI API key (from https://platform.openai.com/api-keys)
# OPENAI_API_KEY=sk-your-openai-api-key

# OpenAI model to use
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo, o1-preview, o1-mini
# Default: gpt-4o
# OPENAI_MODEL=gpt-4o

# OpenAI API endpoint (usually don't need to change this)
# Default: https://api.openai.com/v1/chat/completions
# OPENAI_ENDPOINT=https://api.openai.com/v1/chat/completions

# OpenAI organization ID (optional, for organization-level API keys)
# OPENAI_ORGANIZATION=org-your-org-id

# ==============================================================================
# llama.cpp Configuration (Local GGUF Models)
# ==============================================================================

# llama.cpp server endpoint (default: http://localhost:8080)
# Start with: ./llama-server -m model.gguf --port 8080
# LLAMACPP_ENDPOINT=http://localhost:8080

# Model name (for logging purposes, llama.cpp uses the loaded model)
# LLAMACPP_MODEL=default

# Request timeout in milliseconds (default: 120000)
# LLAMACPP_TIMEOUT_MS=120000

# Optional API key (for secured llama.cpp servers)
# LLAMACPP_API_KEY=your-optional-api-key

# ==============================================================================
# LM Studio Configuration (Local Models with GUI)
# ==============================================================================

# LM Studio server endpoint (default: http://localhost:1234)
# Start LM Studio and load a model, it will start the server automatically
# LMSTUDIO_ENDPOINT=http://localhost:1234

# Model name (for logging purposes, LM Studio uses the loaded model)
# LMSTUDIO_MODEL=default

# Request timeout in milliseconds (default: 120000)
# LMSTUDIO_TIMEOUT_MS=120000

# Optional API key (for secured LM Studio servers)
# LMSTUDIO_API_KEY=your-optional-api-key

# ==============================================================================
# Server Configuration
# ==============================================================================

PORT=8080
LOG_LEVEL=info
WEB_SEARCH_ENDPOINT=http://localhost:8888/search

# Tool execution mode: where to execute tools (Write, Read, Bash, etc.)
# - server: Execute tools on the server (default, for standalone proxy use)
# - passthrough: Return tool calls to CLI for local execution (for Claude Code CLI compatibility)
# TOOL_EXECUTION_MODE=server

# ==============================================================================
# Subagent Configuration (Server Mode - matches Claude Code architecture)
# ==============================================================================

# Enable server-side subagents
# When enabled, the main agent can spawn specialized subagents for complex tasks
# AGENTS_ENABLED=true

# Max concurrent subagents (Claude Code uses up to 10, batched)
# AGENTS_MAX_CONCURRENT=10

# Default model for subagents
# Options: haiku (fast/cheap), sonnet (smart), gpt-4o-mini, gpt-4o
# AGENTS_DEFAULT_MODEL=haiku

# Max steps per subagent
# AGENTS_MAX_STEPS=15

# Timeout per subagent (milliseconds)
# AGENTS_TIMEOUT=120000

# ==============================================================================
# Long-Term Memory System (Titans-Inspired)
# ==============================================================================

# Enable/disable the entire memory system
# When enabled, automatically extracts and retrieves conversation memories
# Default: true
# MEMORY_ENABLED=true

# Maximum number of memories to inject into each request
# Higher = more context but larger prompts
# Default: 5, Range: 1-20
# MEMORY_RETRIEVAL_LIMIT=5

# Minimum surprise score (0.0-1.0) required to store a memory
# Filters out redundant information
# Lower (0.1-0.2) = store more memories
# Higher (0.4-0.5) = only store highly novel information
# Default: 0.3
# MEMORY_SURPRISE_THRESHOLD=0.3

# Auto-delete memories older than this many days
# Prevents database bloat with stale information
# Default: 90
# MEMORY_MAX_AGE_DAYS=90

# Maximum total memories to keep (prunes least important when exceeded)
# Default: 10000
# MEMORY_MAX_COUNT=10000

# Enable importance decay over time (exponential decay)
# Memories become less important as they age
# Default: true
# MEMORY_DECAY_ENABLED=true

# Days for importance to decay by 50% (exponential half-life)
# Shorter (7-14) = prefer recent info
# Longer (60-90) = value historical context
# Default: 30
# MEMORY_DECAY_HALF_LIFE=30

# Include memories with session_id=NULL (global memories) in all sessions
# Global memories = project facts shared across all conversations
# Default: true
# MEMORY_INCLUDE_GLOBAL=true

# Where to inject memories in the prompt
# Options: system (recommended), assistant_preamble
# Default: system
# MEMORY_INJECTION_FORMAT=system

# Enable automatic extraction of memories from assistant responses
# Disable for read-only memory mode
# Default: true
# MEMORY_EXTRACTION_ENABLED=true

# ==============================================================================
# Token Optimization Settings (60-80% Cost Reduction)
# ==============================================================================

# Enable tracking of estimated and actual token usage per request
# Provides visibility into token consumption patterns
# Default: true
# TOKEN_TRACKING_ENABLED=true

# Automatically truncate tool outputs to reduce token usage
# Read: 8k chars, Bash: 30k chars, Grep: 12k chars
# Default: true
# TOOL_TRUNCATION_ENABLED=true

# Memory format: 'compact' (50 tokens) or 'verbose' (250 tokens)
# Compact format saves ~75% of memory-related tokens
# Default: compact
# MEMORY_FORMAT=compact

# Enable deduplication of memories with recent conversation
# Filters out memories already present in last N messages
# Default: true
# MEMORY_DEDUP_ENABLED=true

# Number of recent messages to check for memory deduplication
# Default: 5
# MEMORY_DEDUP_LOOKBACK=5

# System prompt mode: 'dynamic' (adaptive) or 'full' (static)
# Dynamic mode includes only relevant sections based on context
# Saves 500-1000 tokens per request
# Default: dynamic
# SYSTEM_PROMPT_MODE=dynamic

# Tool descriptions: 'minimal' (concise) or 'full' (verbose)
# Minimal descriptions save 200-300 tokens per request
# Default: minimal
# TOOL_DESCRIPTIONS=minimal

# Enable automatic compression of old conversation history
# Summarizes older turns while keeping recent context verbatim
# Default: true
# HISTORY_COMPRESSION_ENABLED=true

# Number of recent turns to keep verbatim (not compressed)
# Default: 10
# HISTORY_KEEP_RECENT_TURNS=10

# Summarize older conversation turns beyond recent window
# Default: true
# HISTORY_SUMMARIZE_OLDER=true

# Warn when approaching token budget (logged, not blocking)
# Default: 100000 tokens
# TOKEN_BUDGET_WARNING=100000

# Maximum token budget per request
# Default: 180000 tokens (leaves buffer for 20k output)
# TOKEN_BUDGET_MAX=180000

# Enforce token budget with adaptive compression
# Automatically applies additional compression when approaching limit
# Default: true
# TOKEN_BUDGET_ENFORCEMENT=true

# ==============================================================================
# Smart Tool Selection (Advanced Token Optimization)
# ==============================================================================

# Smart tool selection is ALWAYS ENABLED and reduces tool count from 12 â†’ 0-6
# tools depending on query complexity
# Potential savings: 500-2000 tokens per request (10-30% reduction)

# Selection strategy
# - heuristic: Balanced approach (recommended, default)
# - aggressive: Maximum token savings, higher risk
# - conservative: Safer, includes more tools when uncertain
# SMART_TOOL_SELECTION_MODE=heuristic

# Maximum token budget for tools per request
# Default: 2500 tokens (~10 tools)
# Lower values force more aggressive filtering
# SMART_TOOL_SELECTION_TOKEN_BUDGET=2500

# ==============================================================================
# Performance & Security
# ==============================================================================

# API retry configuration
# API_RETRY_MAX_RETRIES=3
# API_RETRY_INITIAL_DELAY=1000
# API_RETRY_MAX_DELAY=30000

# Load shedding thresholds (percentage, 0-100)
# LOAD_SHEDDING_HEAP_THRESHOLD=90
# LOAD_SHEDDING_EVENT_LOOP_DELAY=100

# ==============================================================================
# Notes
# ==============================================================================

# For Docker Compose:
# 1. Copy this file to .env
# 2. Fill in your Databricks credentials
# 3. Run: docker-compose up -d
# 4. Wait for Ollama to download the model (first run takes time)
# 5. Access Lynkr at http://localhost:8080

# For standalone Ollama (no Docker):
# 1. Install Ollama: https://ollama.ai/download
# 2. Pull model: ollama pull qwen2.5-coder:latest
# 3. Set OLLAMA_ENDPOINT to http://localhost:11434
# 4. Run Lynkr: npm start
