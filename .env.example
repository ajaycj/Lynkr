# ==============================================================================
# LYNKR CONFIGURATION - All Environment Variables
# Copy this file to .env and fill in your values
#
# FORMAT: Use plain KEY=VALUE syntax (no "export" prefix).
#   Good:  MODEL_PROVIDER=bedrock
#   Bad:   export MODEL_PROVIDER=bedrock
# ==============================================================================

# ==============================================================================
# Model Provider Configuration
# ==============================================================================

# Primary model provider — controls routing when TIER_* vars are NOT configured (static mode).
# When all 4 TIER_* vars are set, tier routing overrides this for request routing.
# Even with tier routing active, MODEL_PROVIDER is still used for:
#   - Startup checks (e.g. waiting for Ollama to be reachable)
#   - Provider discovery API (/v1/providers)
#   - Default provider when a TIER_* value has no "provider:" prefix
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, ollama, llamacpp, lmstudio, bedrock, zai, vertex, moonshot
# Note: PREFER_OLLAMA is deprecated and has no effect. Use TIER_SIMPLE=ollama:<model> instead.
MODEL_PROVIDER=ollama

# ==============================================================================
# Databricks Configuration
# ==============================================================================

# DATABRICKS_API_BASE=https://your-workspace.cloud.databricks.com
# DATABRICKS_API_KEY=dapi1234567890abcdef
# DATABRICKS_ENDPOINT_PATH=/serving-endpoints/your-endpoint/invocations

# ==============================================================================
# Ollama Configuration (Local Models)
# ==============================================================================

# Ollama endpoint
OLLAMA_ENDPOINT=http://localhost:11434

# Ollama timeout in milliseconds
OLLAMA_TIMEOUT_MS=120000

# Ollama model to use (must be compatible with tool calling)
# Options: qwen2.5-coder:latest, llama3.1, mistral-nemo, etc.
OLLAMA_MODEL=qwen2.5-coder:latest

# Max tools for routing to Ollama (requests with more tools go to cloud)
OLLAMA_MAX_TOOLS_FOR_ROUTING=3

# Ollama embeddings configuration (for Cursor @Codebase semantic search)
# Pull model: ollama pull nomic-embed-text
OLLAMA_EMBEDDINGS_MODEL=nomic-embed-text
OLLAMA_EMBEDDINGS_ENDPOINT=http://localhost:11434/api/embeddings

# ==============================================================================
# OpenRouter Configuration (100+ Models via Single API)
# ==============================================================================

# Get your API key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
OPENROUTER_MODEL=openai/gpt-4o-mini
OPENROUTER_EMBEDDINGS_MODEL=openai/text-embedding-ada-002
OPENROUTER_ENDPOINT=https://openrouter.ai/api/v1/chat/completions
OPENROUTER_MAX_TOOLS_FOR_ROUTING=15

# ==============================================================================
# Azure OpenAI Configuration
# ==============================================================================

# Azure OpenAI endpoint (supports both standard and AI Foundry formats)
# Standard: https://<resource>.openai.azure.com
# AI Foundry: https://<resource>.services.ai.azure.com/models/chat/completions?api-version=...
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_API_KEY=your-azure-openai-key
# AZURE_OPENAI_DEPLOYMENT=gpt-4o
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

# ==============================================================================
# Azure Anthropic Configuration
# ==============================================================================

# AZURE_ANTHROPIC_ENDPOINT=https://your-anthropic.openai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-azure-key
# AZURE_ANTHROPIC_VERSION=2023-06-01

# ==============================================================================
# OpenAI Configuration (Direct)
# ==============================================================================

# OPENAI_API_KEY=sk-your-openai-api-key
# OPENAI_MODEL=gpt-4o
# OPENAI_ENDPOINT=https://api.openai.com/v1/chat/completions
# OPENAI_ORGANIZATION=org-your-org-id

# ==============================================================================
# AWS Bedrock Configuration
# ==============================================================================

# IMPORTANT: Lynkr uses Bedrock API Key authentication (Bearer token),
# NOT standard IAM credentials (AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY).
#
# Setup:
#   1. Open AWS Console → Amazon Bedrock → API keys (left sidebar)
#   2. Generate a long-term or short-term API key
#   3. Copy the key (starts with ABSK) and set it below
#   Docs: https://docs.aws.amazon.com/bedrock/latest/userguide/api-keys-use.html
#
# AWS_BEDROCK_API_KEY=ABSK...your-bedrock-api-key
# AWS_BEDROCK_REGION=us-east-1
# AWS_BEDROCK_MODEL_ID=us.anthropic.claude-3-5-sonnet-20241022-v2:0

# ==============================================================================
# llama.cpp Configuration (Local GGUF Models)
# ==============================================================================

LLAMACPP_ENDPOINT=http://localhost:8080
LLAMACPP_MODEL=default
LLAMACPP_TIMEOUT_MS=120000
# LLAMACPP_API_KEY=your-optional-api-key
LLAMACPP_EMBEDDINGS_ENDPOINT=http://localhost:8080/embeddings

# ==============================================================================
# LM Studio Configuration
# ==============================================================================

LMSTUDIO_ENDPOINT=http://localhost:1234
LMSTUDIO_MODEL=default
LMSTUDIO_TIMEOUT_MS=120000
# LMSTUDIO_API_KEY=your-optional-api-key

# ==============================================================================
# Z.AI (Zhipu AI) Configuration - ~1/7 cost of Anthropic
# ==============================================================================

# ZAI_API_KEY=your-zai-api-key
# ZAI_ENDPOINT=https://api.z.ai/api/anthropic/v1/messages
# ZAI_MODEL=GLM-4.7

# ==============================================================================
# Moonshot AI (Kimi) Configuration
# ==============================================================================

# MOONSHOT_API_KEY=your-moonshot-api-key
# MOONSHOT_ENDPOINT=https://api.moonshot.ai/v1/chat/completions
# MOONSHOT_MODEL=kimi-k2-turbo-preview

# ==============================================================================
# Google Vertex AI Configuration (Gemini Models)
# ==============================================================================

# Get your API key from: https://aistudio.google.com/app/apikey
# VERTEX_API_KEY=your-google-api-key
# GOOGLE_API_KEY=your-google-api-key
# VERTEX_MODEL=gemini-2.0-flash

# ==============================================================================
# Fallback Configuration
# ==============================================================================

# Enable automatic fallback when tier provider fails
FALLBACK_ENABLED=false

# Fallback provider (cannot be local: ollama, llamacpp, lmstudio)
# Options: databricks, azure-anthropic, azure-openai, openrouter, openai, bedrock
FALLBACK_PROVIDER=databricks

# ==============================================================================
# Embeddings Provider Override
# ==============================================================================

# Force a specific embeddings provider (default: same as MODEL_PROVIDER)
# Options: ollama, llamacpp, openrouter, openai
# EMBEDDINGS_PROVIDER=ollama

# ==============================================================================
# Server Configuration
# ==============================================================================

PORT=8081
LOG_LEVEL=info
# NODE_ENV=development

# File logging (persistent logs with automatic rotation via pino-roll)
# LOG_FILE_ENABLED=true
# LOG_FILE_PATH=./logs/lynkr.log
# LOG_FILE_LEVEL=debug
# LOG_FILE_FREQUENCY=daily
# LOG_FILE_MAX_FILES=14

# Maximum JSON request body size
REQUEST_JSON_LIMIT=1gb

# Session database path
SESSION_DB_PATH=./data/sessions.db

# Workspace root directory
WORKSPACE_ROOT=/path/to/your/workspace

# ==============================================================================
# Tool Execution Mode
# ==============================================================================

# Where to execute tools
# - server: Execute tools on the proxy server (default)
# - client/passthrough: Return tool calls to CLI for local execution
TOOL_EXECUTION_MODE=server

# Enable/disable automatic tool injection for local models
INJECT_TOOLS_LLAMACPP=true
INJECT_TOOLS_OLLAMA=true

# Suggestion mode model override
# Values: default (same as MODEL_PROVIDER), none (skip), or <model> name
SUGGESTION_MODE_MODEL=default

# ==============================================================================
# Rate Limiting
# ==============================================================================

RATE_LIMIT_ENABLED=true
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX=100
RATE_LIMIT_KEY_BY=session

# ==============================================================================
# Web Search Configuration
# ==============================================================================

WEB_SEARCH_ENDPOINT=http://localhost:8888/search
# WEB_SEARCH_API_KEY=
WEB_SEARCH_ALLOW_ALL=true
# WEB_SEARCH_ALLOWED_HOSTS=localhost,127.0.0.1
WEB_SEARCH_TIMEOUT_MS=10000
WEB_FETCH_BODY_PREVIEW_MAX=10000
WEB_SEARCH_RETRY_ENABLED=true
WEB_SEARCH_MAX_RETRIES=2

# ==============================================================================
# Policy Configuration
# ==============================================================================

POLICY_MAX_STEPS=20
POLICY_MAX_TOOL_CALLS=12
# POLICY_DISALLOWED_TOOLS=dangerous_tool1,dangerous_tool2

# Tool loop guard threshold
POLICY_TOOL_LOOP_THRESHOLD=10

# Git policy
POLICY_GIT_ALLOW_PUSH=false
POLICY_GIT_ALLOW_PULL=true
POLICY_GIT_ALLOW_COMMIT=true
# POLICY_GIT_TEST_COMMAND=npm test
POLICY_GIT_REQUIRE_TESTS=false
# POLICY_GIT_COMMIT_REGEX=^(feat|fix|docs|style|refactor|test|chore):
POLICY_GIT_AUTOSTASH=false

# File access policy
# POLICY_FILE_ALLOWED_PATHS=/path1,/path2
POLICY_FILE_BLOCKED_PATHS=/.env,.env,/etc/passwd,/etc/shadow

# Safe commands
POLICY_SAFE_COMMANDS_ENABLED=true
# POLICY_SAFE_COMMANDS_CONFIG={"allowed":["ls","cat","grep"]}

# ==============================================================================
# Agents Configuration
# ==============================================================================

AGENTS_ENABLED=true
AGENTS_MAX_CONCURRENT=10
AGENTS_DEFAULT_MODEL=haiku
AGENTS_MAX_STEPS=15
AGENTS_TIMEOUT=300000

# ==============================================================================
# MCP Sandbox Configuration
# ==============================================================================

MCP_SANDBOX_ENABLED=true
# MCP_SANDBOX_IMAGE=node:20-alpine
MCP_SANDBOX_RUNTIME=docker
MCP_SANDBOX_CONTAINER_WORKSPACE=/workspace
MCP_SANDBOX_MOUNT_WORKSPACE=true
MCP_SANDBOX_ALLOW_NETWORKING=false
MCP_SANDBOX_NETWORK_MODE=none
MCP_SANDBOX_PASSTHROUGH_ENV=PATH,LANG,LC_ALL,TERM,HOME
# MCP_SANDBOX_EXTRA_MOUNTS=/host/path:/container/path:ro
MCP_SANDBOX_TIMEOUT_MS=20000
# MCP_SANDBOX_USER=node
# MCP_SANDBOX_ENTRYPOINT=/bin/sh
MCP_SANDBOX_REUSE_SESSION=true
MCP_SANDBOX_READ_ONLY_ROOT=false
MCP_SANDBOX_NO_NEW_PRIVILEGES=true
MCP_SANDBOX_DROP_CAPABILITIES=ALL
# MCP_SANDBOX_ADD_CAPABILITIES=NET_BIND_SERVICE
MCP_SANDBOX_MEMORY_LIMIT=512m
MCP_SANDBOX_CPU_LIMIT=1.0
MCP_SANDBOX_PIDS_LIMIT=100

# MCP permissions
MCP_SANDBOX_PERMISSION_MODE=auto
# MCP_SANDBOX_PERMISSION_ALLOW=tool1,tool2
# MCP_SANDBOX_PERMISSION_DENY=tool3,tool4

# MCP server manifest
# MCP_SERVER_MANIFEST=~/.claude/mcp/servers.json
MCP_MANIFEST_DIRS=~/.claude/mcp

# ==============================================================================
# Prompt Cache Configuration
# ==============================================================================

PROMPT_CACHE_ENABLED=true
PROMPT_CACHE_MAX_ENTRIES=1000
PROMPT_CACHE_TTL_MS=300000

# ==============================================================================
# Semantic Response Cache
# ==============================================================================

# Requires an embeddings provider
SEMANTIC_CACHE_ENABLED=false
SEMANTIC_CACHE_THRESHOLD=0.95

# ==============================================================================
# Long-Term Memory System (Titans-Inspired)
# ==============================================================================

MEMORY_ENABLED=true
MEMORY_RETRIEVAL_LIMIT=5
MEMORY_SURPRISE_THRESHOLD=0.3
MEMORY_MAX_AGE_DAYS=90
MEMORY_MAX_COUNT=10000
MEMORY_INCLUDE_GLOBAL=true
MEMORY_INJECTION_FORMAT=system
MEMORY_EXTRACTION_ENABLED=true
MEMORY_DECAY_ENABLED=true
MEMORY_DECAY_HALF_LIFE=30

# ==============================================================================
# Token Optimization Settings (60-80% Cost Reduction)
# ==============================================================================

TOKEN_TRACKING_ENABLED=true
TOOL_TRUNCATION_ENABLED=true
MEMORY_FORMAT=compact
MEMORY_DEDUP_ENABLED=true
MEMORY_DEDUP_LOOKBACK=5
SYSTEM_PROMPT_MODE=dynamic
TOOL_DESCRIPTIONS=minimal
HISTORY_COMPRESSION_ENABLED=true
HISTORY_KEEP_RECENT_TURNS=10
HISTORY_SUMMARIZE_OLDER=true
TOKEN_BUDGET_WARNING=100000
TOKEN_BUDGET_MAX=180000
TOKEN_BUDGET_ENFORCEMENT=true

# TOON JSON->TOON prompt compression (opt-in; for large structured JSON context)
TOON_ENABLED=false
TOON_MIN_BYTES=4096
TOON_FAIL_OPEN=true
TOON_LOG_STATS=true

# ==============================================================================
# Smart Tool Selection
# ==============================================================================

# Selection strategy: heuristic, aggressive, or conservative
SMART_TOOL_SELECTION_MODE=heuristic
SMART_TOOL_SELECTION_TOKEN_BUDGET=2500

# ==============================================================================
# Test Configuration
# ==============================================================================

# WORKSPACE_TEST_COMMAND=npm test
# WORKSPACE_TEST_ARGS=--coverage
WORKSPACE_TEST_TIMEOUT_MS=600000
WORKSPACE_TEST_SANDBOX=auto
WORKSPACE_TEST_COVERAGE_FILES=coverage/coverage-summary.json
# WORKSPACE_TEST_PROFILES=[{"name":"unit","command":"npm test"}]

# ==============================================================================
# Hot Reload Configuration
# ==============================================================================

HOT_RELOAD_ENABLED=true
HOT_RELOAD_DEBOUNCE_MS=1000

# ==============================================================================
# Headroom Context Compression (Sidecar)
# ==============================================================================

# Enable Headroom compression (47-92% token reduction)
HEADROOM_ENABLED=false

# Sidecar endpoint
HEADROOM_ENDPOINT=http://localhost:8787

# Request timeout and minimum tokens
HEADROOM_TIMEOUT_MS=5000
HEADROOM_MIN_TOKENS=500

# Operating mode: audit (observe) or optimize (apply)
HEADROOM_MODE=optimize

# Provider for cache hints: anthropic, openai, google
HEADROOM_PROVIDER=anthropic

# Log level: debug, info, warning, error
HEADROOM_LOG_LEVEL=info

# ==============================================================================
# Headroom Docker Configuration
# ==============================================================================

HEADROOM_DOCKER_ENABLED=true
HEADROOM_DOCKER_IMAGE=lynkr/headroom-sidecar:latest
HEADROOM_DOCKER_CONTAINER_NAME=lynkr-headroom
HEADROOM_DOCKER_PORT=8787
HEADROOM_DOCKER_MEMORY_LIMIT=512m
HEADROOM_DOCKER_CPU_LIMIT=1.0
HEADROOM_DOCKER_RESTART_POLICY=unless-stopped
# HEADROOM_DOCKER_NETWORK=lynkr-network
# HEADROOM_DOCKER_BUILD_CONTEXT=./headroom-sidecar
# HEADROOM_DOCKER_AUTO_BUILD=true

# ==============================================================================
# Headroom Transform Settings
# ==============================================================================

HEADROOM_SMART_CRUSHER=true
HEADROOM_SMART_CRUSHER_MIN_TOKENS=200
HEADROOM_SMART_CRUSHER_MAX_ITEMS=15
HEADROOM_TOOL_CRUSHER=true
HEADROOM_CACHE_ALIGNER=true
HEADROOM_ROLLING_WINDOW=true
HEADROOM_KEEP_TURNS=3

# ==============================================================================
# Headroom CCR (Compress-Cache-Retrieve)
# ==============================================================================

HEADROOM_CCR=true
HEADROOM_CCR_TTL=300

# ==============================================================================
# Headroom LLMLingua (ML Compression - Requires GPU)
# ==============================================================================

HEADROOM_LLMLINGUA=false
HEADROOM_LLMLINGUA_DEVICE=auto

# ==============================================================================
# Tiered Model Routing (Recommended for Cost Optimization)
# ==============================================================================
# When all 4 TIER_* vars are set, they OVERRIDE MODEL_PROVIDER for routing.
# Each request is scored for complexity (0-100) and routed to the matching tier:
#   SIMPLE (0-25) → cheap/local models    COMPLEX (51-75) → capable cloud models
#   MEDIUM (26-50) → mid-range models     REASONING (76-100) → best available
#
# Format: TIER_<LEVEL>=provider:model
# All 4 tiers must be configured to enable tiered routing.
# If any are missing, tiered routing is disabled and MODEL_PROVIDER is used directly.
#
# Supported providers: ollama, openai, azure-openai, openrouter,
#                      databricks, bedrock, vertex, zai, moonshot, llamacpp, lmstudio
#
TIER_SIMPLE=ollama:llama3.2
TIER_MEDIUM=openrouter:openai/gpt-4o-mini
TIER_COMPLEX=azure-openai:gpt-4o
TIER_REASONING=azure-openai:gpt-4o
