services:
  # Lynkr proxy service
  lynkr:
    build: .
    container_name: lynkr
    image: lynkr:2.0.0
    ports:
      - "8081:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      # ============================================================
      # PRIMARY MODEL PROVIDER
      # ============================================================
      # Options: ollama, databricks, azure-openai, azure-anthropic, openrouter, bedrock, llamacpp, lmstudio, openai
      # - ollama: Local models (free, private, offline)
      # - databricks: Claude Sonnet 4.5, Opus 4.5 (production)
      # - azure-openai: GPT-4o, GPT-5, o1, o3 (Azure integration)
      # - azure-anthropic: Claude models via Azure
      # - openrouter: 100+ models (flexible, cost-effective)
      # - bedrock: AWS Bedrock (Claude, Titan, Llama, etc.)
      # - llamacpp: Local GGUF models
      # - lmstudio: LM Studio local models
      # - openai: Direct OpenAI API
      MODEL_PROVIDER: ${MODEL_PROVIDER:-ollama}

      # ============================================================
      # TOOL EXECUTION MODE
      # ============================================================
      # Options: server (default), client (passthrough mode)
      # - server: Tools execute on proxy server
      # - client: Tools execute on Claude Code CLI (client-side)
      TOOL_EXECUTION_MODE: ${TOOL_EXECUTION_MODE:-server}

      # ============================================================
      # OLLAMA CONFIGURATION (Local Models)
      # ============================================================
      # Recommended models for tool calling:
      # - llama3.1:8b (good balance)
      # - llama3.2 (latest)
      # - qwen2.5:14b (strong reasoning, 7b struggles with tools)
      # - mistral:7b-instruct (fast and capable)
      # Tier-based routing (set all 4 to enable)
      TIER_SIMPLE: ${TIER_SIMPLE:-}
      TIER_MEDIUM: ${TIER_MEDIUM:-}
      TIER_COMPLEX: ${TIER_COMPLEX:-}
      TIER_REASONING: ${TIER_REASONING:-}
#      OLLAMA_ENDPOINT: http://ollama:11434
      OLLAMA_ENDPOINT: http://host.docker.internal:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.1:8b}
      OLLAMA_MAX_TOOLS_FOR_ROUTING: ${OLLAMA_MAX_TOOLS_FOR_ROUTING:-3}
      # Ollama Embeddings (for Cursor @Codebase search)
      OLLAMA_EMBEDDINGS_MODEL: ${OLLAMA_EMBEDDINGS_MODEL:-nomic-embed-text}
#      OLLAMA_EMBEDDINGS_ENDPOINT: ${OLLAMA_EMBEDDINGS_ENDPOINT:-http://ollama:11434/api/embeddings}
      OLLAMA_EMBEDDINGS_ENDPOINT: ${OLLAMA_EMBEDDINGS_ENDPOINT:-http://host.docker.internal:11434/api/embeddings}

      # ============================================================
      # OPENROUTER CONFIGURATION
      # ============================================================
      # Get API key from: https://openrouter.ai/keys
      # Popular models: openai/gpt-4o-mini, anthropic/claude-3.5-sonnet
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}
      OPENROUTER_MODEL: ${OPENROUTER_MODEL:-amazon/nova-2-lite-v1:free}
      OPENROUTER_EMBEDDINGS_MODEL: ${OPENROUTER_EMBEDDINGS_MODEL:-openai/text-embedding-ada-002}
      OPENROUTER_ENDPOINT: ${OPENROUTER_ENDPOINT:-https://openrouter.ai/api/v1/chat/completions}
      OPENROUTER_MAX_TOOLS_FOR_ROUTING: ${OPENROUTER_MAX_TOOLS_FOR_ROUTING:-15}

      # ============================================================
      # AZURE OPENAI CONFIGURATION
      # ============================================================
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT:-}
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:-}
      AZURE_OPENAI_DEPLOYMENT: ${AZURE_OPENAI_DEPLOYMENT:-gpt-4o}
      AZURE_OPENAI_API_VERSION: ${AZURE_OPENAI_API_VERSION:-2024-08-01-preview}

      # ============================================================
      # HYBRID ROUTING & FALLBACK
      # ============================================================
      FALLBACK_ENABLED: ${FALLBACK_ENABLED:-true}
      FALLBACK_PROVIDER: ${FALLBACK_PROVIDER:-databricks}

      # ============================================================
      # DATABRICKS CONFIGURATION
      # ============================================================
      DATABRICKS_API_BASE: ${DATABRICKS_API_BASE:-https://example.cloud.databricks.com}
      DATABRICKS_API_KEY: ${DATABRICKS_API_KEY:-replace-with-databricks-pat}

      # ============================================================
      # AZURE ANTHROPIC CONFIGURATION (OPTIONAL)
      # ============================================================
      AZURE_ANTHROPIC_ENDPOINT: ${AZURE_ANTHROPIC_ENDPOINT:-}
      AZURE_ANTHROPIC_API_KEY: ${AZURE_ANTHROPIC_API_KEY:-}
      AZURE_ANTHROPIC_VERSION: ${AZURE_ANTHROPIC_VERSION:-2023-06-01}

      # ============================================================
      # AWS BEDROCK CONFIGURATION (OPTIONAL)
      # ============================================================
      AWS_BEDROCK_API_KEY: ${AWS_BEDROCK_API_KEY:-}
      AWS_BEDROCK_REGION: ${AWS_BEDROCK_REGION:-us-east-1}
      AWS_BEDROCK_MODEL_ID: ${AWS_BEDROCK_MODEL_ID:-anthropic.claude-3-5-sonnet-20241022-v2:0}

      # ============================================================
      # LLAMA.CPP CONFIGURATION (OPTIONAL)
      # ============================================================
      LLAMACPP_ENDPOINT: ${LLAMACPP_ENDPOINT:-http://localhost:8080}
      LLAMACPP_MODEL: ${LLAMACPP_MODEL:-default}
      LLAMACPP_EMBEDDINGS_ENDPOINT: ${LLAMACPP_EMBEDDINGS_ENDPOINT:-http://localhost:8080/embeddings}
      LLAMACPP_TIMEOUT_MS: ${LLAMACPP_TIMEOUT_MS:-120000}

      # ============================================================
      # LM STUDIO CONFIGURATION (OPTIONAL)
      # ============================================================
      LMSTUDIO_ENDPOINT: ${LMSTUDIO_ENDPOINT:-http://localhost:1234}
      LMSTUDIO_MODEL: ${LMSTUDIO_MODEL:-default}
      LMSTUDIO_TIMEOUT_MS: ${LMSTUDIO_TIMEOUT_MS:-120000}

      # ============================================================
      # OPENAI CONFIGURATION (OPTIONAL)
      # ============================================================
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o}
      OPENAI_ENDPOINT: ${OPENAI_ENDPOINT:-https://api.openai.com/v1/chat/completions}

      # ============================================================
      # Z.AI CONFIGURATION (OPTIONAL)
      # ============================================================
      ZAI_API_KEY: ${ZAI_API_KEY:-}
      ZAI_ENDPOINT: ${ZAI_ENDPOINT:-https://api.z.ai/api/anthropic/v1/messages}
      ZAI_MODEL: ${ZAI_MODEL:-GLM-4.7}

      # ============================================================
      # GOOGLE VERTEX AI CONFIGURATION (OPTIONAL)
      # ============================================================
      VERTEX_API_KEY: ${VERTEX_API_KEY:-}
      VERTEX_MODEL: ${VERTEX_MODEL:-gemini-2.0-flash}

      # ============================================================
      # MOONSHOT AI (KIMI) CONFIGURATION (OPTIONAL)
      # ============================================================
      MOONSHOT_API_KEY: ${MOONSHOT_API_KEY:-}
      MOONSHOT_ENDPOINT: ${MOONSHOT_ENDPOINT:-https://api.moonshot.ai/v1/chat/completions}
      MOONSHOT_MODEL: ${MOONSHOT_MODEL:-kimi-k2-thinking}

      # ============================================================
      # EMBEDDINGS PROVIDER OVERRIDE (OPTIONAL)
      # ============================================================
      EMBEDDINGS_PROVIDER: ${EMBEDDINGS_PROVIDER:-}

      # ============================================================
      # SERVER CONFIGURATION
      # ============================================================
      PORT: ${PORT:-8081}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      NODE_ENV: ${NODE_ENV:-production}
      REQUEST_JSON_LIMIT: ${REQUEST_JSON_LIMIT:-1gb}
      SESSION_DB_PATH: /app/data/sessions.db
      WEB_SEARCH_ENDPOINT: ${WEB_SEARCH_ENDPOINT:-http://searxng:8888/search}
      WORKSPACE_ROOT: /workspace

      # ============================================================
      # FILE LOGGING (pino-roll rotation)
      # ============================================================
      LOG_FILE_ENABLED: ${LOG_FILE_ENABLED:-false}
      LOG_FILE_PATH: /app/logs/lynkr.log
      LOG_FILE_LEVEL: ${LOG_FILE_LEVEL:-debug}
      LOG_FILE_FREQUENCY: ${LOG_FILE_FREQUENCY:-daily}
      LOG_FILE_MAX_FILES: ${LOG_FILE_MAX_FILES:-14}

      # ============================================================
      # TOOL INJECTION & SUGGESTION MODE
      # ============================================================
      INJECT_TOOLS_LLAMACPP: ${INJECT_TOOLS_LLAMACPP:-true}
      INJECT_TOOLS_OLLAMA: ${INJECT_TOOLS_OLLAMA:-true}
      SUGGESTION_MODE_MODEL: ${SUGGESTION_MODE_MODEL:-default}

      # ============================================================
      # RATE LIMITING
      # ============================================================
      RATE_LIMIT_ENABLED: ${RATE_LIMIT_ENABLED:-true}
      RATE_LIMIT_WINDOW_MS: ${RATE_LIMIT_WINDOW_MS:-60000}
      RATE_LIMIT_MAX: ${RATE_LIMIT_MAX:-100}
      RATE_LIMIT_KEY_BY: ${RATE_LIMIT_KEY_BY:-session}

      # ============================================================
      # WEB SEARCH
      # ============================================================
      WEB_SEARCH_ALLOW_ALL: ${WEB_SEARCH_ALLOW_ALL:-true}
      WEB_SEARCH_TIMEOUT_MS: ${WEB_SEARCH_TIMEOUT_MS:-10000}
      WEB_FETCH_BODY_PREVIEW_MAX: ${WEB_FETCH_BODY_PREVIEW_MAX:-10000}
      WEB_SEARCH_RETRY_ENABLED: ${WEB_SEARCH_RETRY_ENABLED:-true}
      WEB_SEARCH_MAX_RETRIES: ${WEB_SEARCH_MAX_RETRIES:-2}

      # ============================================================
      # TINYFISH AI BROWSER AUTOMATION (WebAgent tool)
      # ============================================================
      # Get your API key from: https://tinyfish.ai
      TINYFISH_API_KEY: ${TINYFISH_API_KEY:-}
      TINYFISH_ENDPOINT: ${TINYFISH_ENDPOINT:-https://agent.tinyfish.ai/v1/automation/run-sse}
      TINYFISH_BROWSER_PROFILE: ${TINYFISH_BROWSER_PROFILE:-lite}
      TINYFISH_TIMEOUT_MS: ${TINYFISH_TIMEOUT_MS:-120000}
      TINYFISH_PROXY_ENABLED: ${TINYFISH_PROXY_ENABLED:-false}
      TINYFISH_PROXY_COUNTRY: ${TINYFISH_PROXY_COUNTRY:-US}

      # ============================================================
      # POLICY CONFIGURATION
      # ============================================================
      POLICY_MAX_STEPS: ${POLICY_MAX_STEPS:-20}
      POLICY_MAX_TOOL_CALLS: ${POLICY_MAX_TOOL_CALLS:-12}
      POLICY_TOOL_LOOP_THRESHOLD: ${POLICY_TOOL_LOOP_THRESHOLD:-10}
      POLICY_GIT_ALLOW_PUSH: ${POLICY_GIT_ALLOW_PUSH:-false}
      POLICY_GIT_ALLOW_PULL: ${POLICY_GIT_ALLOW_PULL:-true}
      POLICY_GIT_ALLOW_COMMIT: ${POLICY_GIT_ALLOW_COMMIT:-true}
      POLICY_GIT_REQUIRE_TESTS: ${POLICY_GIT_REQUIRE_TESTS:-false}
      POLICY_GIT_AUTOSTASH: ${POLICY_GIT_AUTOSTASH:-false}
      POLICY_FILE_BLOCKED_PATHS: ${POLICY_FILE_BLOCKED_PATHS:-/.env,.env,/etc/passwd,/etc/shadow}
      POLICY_SAFE_COMMANDS_ENABLED: ${POLICY_SAFE_COMMANDS_ENABLED:-true}

      # ============================================================
      # AGENTS CONFIGURATION
      # ============================================================
      AGENTS_ENABLED: ${AGENTS_ENABLED:-true}
      AGENTS_MAX_CONCURRENT: ${AGENTS_MAX_CONCURRENT:-10}
      AGENTS_DEFAULT_MODEL: ${AGENTS_DEFAULT_MODEL:-haiku}
      AGENTS_MAX_STEPS: ${AGENTS_MAX_STEPS:-15}
      AGENTS_TIMEOUT: ${AGENTS_TIMEOUT:-300000}

      # ============================================================
      # PROMPT & SEMANTIC CACHE
      # ============================================================
      PROMPT_CACHE_ENABLED: ${PROMPT_CACHE_ENABLED:-true}
      PROMPT_CACHE_MAX_ENTRIES: ${PROMPT_CACHE_MAX_ENTRIES:-1000}
      PROMPT_CACHE_TTL_MS: ${PROMPT_CACHE_TTL_MS:-300000}
      SEMANTIC_CACHE_ENABLED: ${SEMANTIC_CACHE_ENABLED:-false}
      SEMANTIC_CACHE_THRESHOLD: ${SEMANTIC_CACHE_THRESHOLD:-0.95}

      # ============================================================
      # PRODUCTION HARDENING
      # ============================================================
      CIRCUIT_BREAKER_FAILURE_THRESHOLD: ${CIRCUIT_BREAKER_FAILURE_THRESHOLD:-5}
      CIRCUIT_BREAKER_TIMEOUT: ${CIRCUIT_BREAKER_TIMEOUT:-60000}
      LOAD_SHEDDING_MEMORY_THRESHOLD: ${LOAD_SHEDDING_MEMORY_THRESHOLD:-0.85}

      # ============================================================
      # LONG-TERM MEMORY (Titans-inspired)
      # ============================================================
      MEMORY_ENABLED: ${MEMORY_ENABLED:-true}
      MEMORY_RETRIEVAL_LIMIT: ${MEMORY_RETRIEVAL_LIMIT:-5}
      MEMORY_SURPRISE_THRESHOLD: ${MEMORY_SURPRISE_THRESHOLD:-0.3}
      MEMORY_MAX_AGE_DAYS: ${MEMORY_MAX_AGE_DAYS:-90}
      MEMORY_MAX_COUNT: ${MEMORY_MAX_COUNT:-10000}
      MEMORY_INCLUDE_GLOBAL: ${MEMORY_INCLUDE_GLOBAL:-true}
      MEMORY_INJECTION_FORMAT: ${MEMORY_INJECTION_FORMAT:-system}
      MEMORY_EXTRACTION_ENABLED: ${MEMORY_EXTRACTION_ENABLED:-true}
      MEMORY_DECAY_ENABLED: ${MEMORY_DECAY_ENABLED:-true}
      MEMORY_DECAY_HALF_LIFE: ${MEMORY_DECAY_HALF_LIFE:-30}

      # ============================================================
      # TOKEN OPTIMIZATION (60-80% cost reduction)
      # ============================================================
      TOKEN_TRACKING_ENABLED: ${TOKEN_TRACKING_ENABLED:-true}
      TOOL_TRUNCATION_ENABLED: ${TOOL_TRUNCATION_ENABLED:-true}
      MEMORY_FORMAT: ${MEMORY_FORMAT:-compact}
      MEMORY_DEDUP_ENABLED: ${MEMORY_DEDUP_ENABLED:-true}
      MEMORY_DEDUP_LOOKBACK: ${MEMORY_DEDUP_LOOKBACK:-5}
      SYSTEM_PROMPT_MODE: ${SYSTEM_PROMPT_MODE:-dynamic}
      TOOL_DESCRIPTIONS: ${TOOL_DESCRIPTIONS:-minimal}
      HISTORY_COMPRESSION_ENABLED: ${HISTORY_COMPRESSION_ENABLED:-true}
      HISTORY_KEEP_RECENT_TURNS: ${HISTORY_KEEP_RECENT_TURNS:-10}
      HISTORY_SUMMARIZE_OLDER: ${HISTORY_SUMMARIZE_OLDER:-true}
      TOKEN_BUDGET_WARNING: ${TOKEN_BUDGET_WARNING:-100000}
      TOKEN_BUDGET_MAX: ${TOKEN_BUDGET_MAX:-180000}
      TOKEN_BUDGET_ENFORCEMENT: ${TOKEN_BUDGET_ENFORCEMENT:-true}

      # ============================================================
      # SMART TOOL SELECTION
      # ============================================================
      SMART_TOOL_SELECTION_MODE: ${SMART_TOOL_SELECTION_MODE:-heuristic}
      SMART_TOOL_SELECTION_TOKEN_BUDGET: ${SMART_TOOL_SELECTION_TOKEN_BUDGET:-2500}

      # ============================================================
      # HOT RELOAD
      # ============================================================
      HOT_RELOAD_ENABLED: ${HOT_RELOAD_ENABLED:-true}
      HOT_RELOAD_DEBOUNCE_MS: ${HOT_RELOAD_DEBOUNCE_MS:-1000}

      # ============================================================
      # HEADROOM CONTEXT COMPRESSION (OPTIONAL)
      # ============================================================
      # Provides 47-92% token reduction through intelligent compression
      HEADROOM_ENABLED: ${HEADROOM_ENABLED:-false}
      HEADROOM_ENDPOINT: ${HEADROOM_ENDPOINT:-http://headroom:8787}
      HEADROOM_TIMEOUT_MS: ${HEADROOM_TIMEOUT_MS:-5000}
      HEADROOM_MIN_TOKENS: ${HEADROOM_MIN_TOKENS:-500}
      HEADROOM_MODE: ${HEADROOM_MODE:-optimize}
      # Disable Docker management - we use docker-compose instead
      HEADROOM_DOCKER_ENABLED: "false"
      # Transform settings
      HEADROOM_SMART_CRUSHER: ${HEADROOM_SMART_CRUSHER:-true}
      HEADROOM_TOOL_CRUSHER: ${HEADROOM_TOOL_CRUSHER:-true}
      HEADROOM_CACHE_ALIGNER: ${HEADROOM_CACHE_ALIGNER:-true}
      HEADROOM_ROLLING_WINDOW: ${HEADROOM_ROLLING_WINDOW:-true}
      HEADROOM_KEEP_TURNS: ${HEADROOM_KEEP_TURNS:-3}
      HEADROOM_CCR: ${HEADROOM_CCR:-true}
      HEADROOM_CCR_TTL: ${HEADROOM_CCR_TTL:-300}
      HEADROOM_LLMLINGUA: ${HEADROOM_LLMLINGUA:-false}

      # ============================================================
      # TIERED MODEL ROUTING (OPTIONAL)
      # ============================================================
      # Format: TIER_<LEVEL>=provider:model
      # All 4 must be set to enable tiered routing
      # TIER_SIMPLE: ${TIER_SIMPLE:-}
      # TIER_MEDIUM: ${TIER_MEDIUM:-}
      # TIER_COMPLEX: ${TIER_COMPLEX:-}
      # TIER_REASONING: ${TIER_REASONING:-}

    volumes:
      - ./data:/app/data    # Persist SQLite databases
      - ./logs:/app/logs    # Persist log files
      - .:/workspace        # Mount workspace
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      - "com.lynkr.version=1.0.1"
      - "com.lynkr.description=Claude Code proxy with multi-provider support"
    # Uncomment to set resource limits
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '2'
    #       memory: 2G
    #     reservations:
    #       cpus: '0.5'
    #       memory: 512M

  # Ollama service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles:
      - ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama  # Persist downloaded models
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "com.lynkr.service=ollama"
      - "com.lynkr.description=Local LLM runtime"
    # Uncomment for NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Optional: Ollama Web UI (if you want a visual interface)
  # ollama-webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: ollama-webui
  #   ports:
  #     - "3000:8080"
  #   environment:
  #     OLLAMA_BASE_URL: http://ollama:11434
  #   volumes:
  #     - ollama-webui-data:/app/backend/data
  #   depends_on:
  #     ollama:
  #       condition: service_healthy
  #   restart: unless-stopped
  #   networks:
  #     - lynkr-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # Local searxng search service (web search provider)
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    ports:
      - "8888:8080"
    volumes:
      - searxng-data:/etc/searxng
#    environment:
      # you can add SEARX settings here if needed (e.g. timezone, settings.yml mount, etc.)
      # see https://docs.searxng.org/admin/installation-docker.html#environment-variables
    dns:
      - 8.8.8.8
      - 1.1.1.1
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Headroom context compression sidecar (47-92% token reduction)
  headroom:
    image: lynkr/headroom-sidecar:latest
    container_name: lynkr-headroom
    profiles:
      - headroom
    build:
      context: ./headroom-sidecar
      dockerfile: Dockerfile
    ports:
      - "8787:8787"
    environment:
      HEADROOM_HOST: "0.0.0.0"
      HEADROOM_PORT: "8787"
      HEADROOM_LOG_LEVEL: ${HEADROOM_LOG_LEVEL:-info}
      HEADROOM_MODE: ${HEADROOM_MODE:-optimize}
      HEADROOM_PROVIDER: ${HEADROOM_PROVIDER:-anthropic}
      # Transforms
      HEADROOM_SMART_CRUSHER: ${HEADROOM_SMART_CRUSHER:-true}
      HEADROOM_SMART_CRUSHER_MIN_TOKENS: ${HEADROOM_SMART_CRUSHER_MIN_TOKENS:-200}
      HEADROOM_SMART_CRUSHER_MAX_ITEMS: ${HEADROOM_SMART_CRUSHER_MAX_ITEMS:-15}
      HEADROOM_TOOL_CRUSHER: ${HEADROOM_TOOL_CRUSHER:-true}
      HEADROOM_CACHE_ALIGNER: ${HEADROOM_CACHE_ALIGNER:-true}
      HEADROOM_ROLLING_WINDOW: ${HEADROOM_ROLLING_WINDOW:-true}
      HEADROOM_KEEP_TURNS: ${HEADROOM_KEEP_TURNS:-3}
      # CCR
      HEADROOM_CCR: ${HEADROOM_CCR:-true}
      HEADROOM_CCR_TTL: ${HEADROOM_CCR_TTL:-300}
      # LLMLingua (optional, requires GPU)
      HEADROOM_LLMLINGUA: ${HEADROOM_LLMLINGUA:-false}
      HEADROOM_LLMLINGUA_DEVICE: ${HEADROOM_LLMLINGUA_DEVICE:-auto}
    volumes:
      - headroom-data:/app/data
    restart: unless-stopped
    networks:
      - lynkr-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8787/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "com.lynkr.service=headroom"
      - "com.lynkr.description=Context compression sidecar"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    # Uncomment for GPU support (LLMLingua)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama-data:
    driver: local
  # ollama-webui-data:
  #   driver: local
  searxng-data:
    driver: local
  headroom-data:
    driver: local

networks:
  lynkr-network:
    driver: bridge
